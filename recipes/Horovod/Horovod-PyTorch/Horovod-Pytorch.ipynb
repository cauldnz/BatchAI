{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Distributed via Horovod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This recipe shows how to run the [Horovod](https://github.com/uber/horovod) distributed training framework for a [PyTorch](https://pytorch.org/) model using Batch AI.\n",
    "\n",
    "Currently Batch AI has no native support for Horovod framework, but it's easy to run it using customtoolkit and job preparation command line.\n",
    "\n",
    "\n",
    "## Details\n",
    "\n",
    "- Standard Horovod [pytorch_mnist.py](https://raw.githubusercontent.com/uber/horovod/master/examples/pytorch_mnist.py) example will be used;\n",
    "- pytorch_mnist.py downloads training data on its own during execution;\n",
    "- The job will be run on the Horovod provded container [uber/horovod:0.13.4-tf1.8.0-torch0.4.0-py3.5](https://hub.docker.com/r/uber/horovod/tags/);\n",
    "- Horovod and all other dependencies are already installed in the Docker container. See the Tensorflow [Horovod sample](Horovod.ipynb) for guidance on installing at startup. For a production scenario you'd probably want to trim the docker container back to improve job startup performance.\n",
    "- Standard output of the job will be stored on Azure File Share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions used by different notebooks\n",
    "sys.path.append('../../')\n",
    "import utilities\n",
    "\n",
    "cfg = utilities.Configuration('../../configuration.json')\n",
    "client = utilities.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Dataset and Script in Azure Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account. This share will be populated with sample scripts and will contain job's output.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_share(azure_file_share_name, fail_on_exist=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script and Configure the Input Directories\n",
    "\n",
    "- Download original sample script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/uber/horovod/master/examples/pytorch_mnist.py ...Done\n"
     ]
    }
   ],
   "source": [
    "sample_script_url = 'https://raw.githubusercontent.com/uber/horovod/master/examples/pytorch_mnist.py'\n",
    "utilities.download_file(sample_script_url, 'pytorch_mnist.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a folder in the file share and upload the sample script to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dir = 'horovod_samples'\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, samples_dir, fail_on_exist=False)\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, samples_dir, 'pytorch_mnist.py', 'pytorch_mnist.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Azure Batch AI Compute Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "\n",
    "- For this example we will use a GPU cluster of `STANDARD_NC6` nodes. Number of nodes in the cluster is configured with `nodes_count` variable;\n",
    "- We will mount file share at folder with name `afs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/afs`;\n",
    "- We will call the cluster `nc6`.\n",
    "- We will use Low Priority VMs\n",
    "\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_file_share = 'afs'\n",
    "nodes_count = 1\n",
    "cluster_name = 'nc12'\n",
    "\n",
    "volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url = 'https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share)\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size=\"STANDARD_NC12\",\n",
    "    vm_priority=\"lowpriority\",\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        mount_volumes=volumes\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "utilities.py contains a helper function allowing to wait for the cluster to become available - all nodes are allocated and finished preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n"
     ]
    }
   ],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Azure Batch AI Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Input Directories\n",
    "\n",
    "The job needs to know where to find mnist_replica.py and input MNIST dataset. We will create two input directories for this:\n",
    "The job needs to know where to find train_mnist.py script (the chainer will download MNIST dataset on its own). So, we will configure an input directory for the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directories = [\n",
    "    models.InputDirectory(\n",
    "        id='SCRIPTS',\n",
    "        path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, samples_dir))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job will be able to reference those directories using ```$AZ_BATCHAI_INPUT_SCRIPTS``` environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Output Directories\n",
    "We will store standard and error output of the job in File Share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_output_path_prefix = '$AZ_BATCHAI_MOUNT_ROOT/{0}'.format(azure_file_share)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model output will be stored in File Share:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directories = [\n",
    "    models.OutputDirectory(\n",
    "        id='MODEL',\n",
    "        path_prefix='$AZ_BATCHAI_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "        path_suffix='Models')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "\n",
    "- Will use configured previously input and output directories;\n",
    "- We will use custom toolkit job to run tensorflow_mnist.py on multiple nodes (use node_count parameter to specify number of nodes). Note, Batch AI will create a hostfile for the job, it can be found via ```$AZ_BATCHAI_MPI_HOST_FILE``` environment variable;\n",
    "- Horovod framework will be installed by job preparation command line;\n",
    "- Will output standard output and error streams to file share.\n",
    "\n",
    "You can delete ```container_settings``` from the job definition to run the job directly on host DSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = models.JobCreateParameters(\n",
    "     location=cfg.location,\n",
    "     cluster=models.ResourceId(id=cluster.id),\n",
    "     node_count=1,\n",
    "     input_directories=input_directories,\n",
    "     output_directories=output_directories,\n",
    "     std_out_err_path_prefix=std_output_path_prefix,\n",
    "     container_settings=models.ContainerSettings(\n",
    "         image_source_registry=models.ImageSourceRegistry(image='uber/horovod:0.13.4-tf1.8.0-torch0.4.0-py3.5')),\n",
    "     custom_toolkit_settings = models.CustomToolkitSettings(\n",
    "         command_line='tail $AZ_BATCHAI_MPI_HOST_FILE; mpirun  -mca btl_tcp_if_exclude docker0,lo --allow-run-as-root --tag-output --hostfile $AZ_BATCHAI_MPI_HOST_FILE python $AZ_BATCHAI_INPUT_SCRIPTS/pytorch_mnist.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Job: horovod_pytorch_06_12_2018_024934\n"
     ]
    }
   ],
   "source": [
    "job_name = datetime.utcnow().strftime('horovod_pytorch_%m_%d_%Y_%H%M%S')\n",
    "job = client.jobs.create(cfg.resource_group, job_name, parameters).result()\n",
    "print('Created Job: {}'.format(job.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enough idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stderr.txt.\n",
    "\n",
    "**Note** Execution may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: AllocationState.steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 1; Preparing: 0; Leaving: 0\n",
      "Job state: running ExitCode: None\n",
      "Waiting for job output to become available...\n",
      "[1,1]<stddiag>:[28247c31031f45d7acb4b393b8082300000000:00088] Read -1, expected 10000, errno = 1\n",
      "[1,1]<stddiag>:[28247c31031f45d7acb4b393b8082300000000:00088] Read -1, expected 32000, errno = 1\n",
      "[1,0]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[1,0]<stderr>:  return F.log_softmax(x)\n",
      "[1,0]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[1,0]<stderr>:  100. * batch_idx / len(train_loader), loss.data[0]))\n",
      "[1,1]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:80: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[1,1]<stderr>:  return F.log_softmax(x)\n",
      "[1,1]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:116: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[1,1]<stderr>:  100. * batch_idx / len(train_loader), loss.data[0]))\n",
      "[1,1]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "[1,1]<stderr>:  data, target = Variable(data, volatile=True), Variable(target)\n",
      "[1,1]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:135: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[1,1]<stderr>:  test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
      "[1,0]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "[1,0]<stderr>:  data, target = Variable(data, volatile=True), Variable(target)\n",
      "[1,0]<stderr>:/mnt/batch/tasks/shared/LS_root/mounts/afs/horovod_samples/pytorch_mnist.py:135: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "[1,0]<stderr>:  test_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
      "Job state: succeeded ExitCode: 0\n"
     ]
    }
   ],
   "source": [
    "utilities.wait_for_job_completion(client, cfg.resource_group, job_name, cluster_name, 'stdouterr', 'stderr.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download stdout.txt and stderr.txt files for the Job and job preparation command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, job_name,\n",
    "                                      models.JobsListOutputFilesOptions(outputdirectoryid='stdouterr')) \n",
    "for f in list(files):\n",
    "    if f.download_url:\n",
    "        utilities.download_file(f.download_url, f.name)\n",
    "print('All files downloaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(cfg.resource_group, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_= client.clusters.delete(cfg.resource_group, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
